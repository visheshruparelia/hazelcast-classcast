/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package hazelcast.classcast;

import com.hazelcast.client.HazelcastClient;
import com.hazelcast.client.config.ClientConfig;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.function.FunctionEx;
import com.hazelcast.jet.Job;
import com.hazelcast.jet.JobAlreadyExistsException;
import com.hazelcast.jet.config.JobConfig;
import com.hazelcast.jet.core.ProcessorMetaSupplier;
import com.hazelcast.jet.kafka.impl.StreamKafkaP;
import com.hazelcast.jet.pipeline.Pipeline;
import com.hazelcast.jet.pipeline.Sinks;
import com.hazelcast.jet.pipeline.StreamSource;
import com.hazelcast.jet.pipeline.StreamStage;
import demo.Emp;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.header.Headers;
import org.apache.kafka.common.serialization.ByteArrayDeserializer;

import java.util.Collections;
import java.util.Map;
import java.util.Properties;

import static com.hazelcast.jet.Util.entry;
import static com.hazelcast.jet.pipeline.Sources.streamFromProcessorWithWatermarks;
import static io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig.AUTO_REGISTER_SCHEMAS;
import static io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY;
import static io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;
import static io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY;
import static io.confluent.kafka.serializers.KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG;
import static io.confluent.kafka.serializers.KafkaAvroSerializerConfig.AVRO_REMOVE_JAVA_PROPS_CONFIG;

public class App {
    public static void main(String[] args) {
        Pipeline p = getPipeline(getKafkaConsumerProperties(), getSerDeProperties());
        System.out.println(p.toDag().toDotString());
        JobConfig jobConfig = createJobConfig("testPipeline");
        ClientConfig cg = new ClientConfig();
        cg.getNetworkConfig().addAddress("127.0.0.1:5701");
        cg.setClusterName("dev");
        HazelcastInstance hazelcastInstance = HazelcastClient.newHazelcastClient(cg);
        jobConfig.addPackage( "hazelcast.classcast", "io.confluent", "org.apache", "demo");
        Job job;
        try{
            job = hazelcastInstance.getJet().newJob(p, jobConfig);
        } catch (JobAlreadyExistsException e) {
            hazelcastInstance.getJet().getJob("testPipeline").cancel();
            job = hazelcastInstance.getJet().newJob(p, jobConfig);
        }
    }

    static JobConfig createJobConfig(String workflowName) {
        return new JobConfig()
                .setStoreMetricsAfterJobCompletion(true)
                .setSnapshotIntervalMillis(1000)
                .setName(workflowName);
    }

    private static Map<String, Object> getSerDeProperties() {
        return Map.of(SCHEMA_REGISTRY_URL_CONFIG, "http://schemaregistry:8081",
                AUTO_REGISTER_SCHEMAS, false,
                SPECIFIC_AVRO_READER_CONFIG, true,
                KEY_SUBJECT_NAME_STRATEGY, "io.confluent.kafka.serializers.subject.RecordNameStrategy",
                VALUE_SUBJECT_NAME_STRATEGY, "io.confluent.kafka.serializers.subject.RecordNameStrategy",
                AVRO_REMOVE_JAVA_PROPS_CONFIG, true);
    }

    private static Properties getKafkaConsumerProperties() {
        Properties consumerProperties = new Properties();
        consumerProperties.setProperty("bootstrap.servers", "kafka:9092");
        consumerProperties.setProperty("key.deserializer", ByteArrayDeserializer.class.getCanonicalName());
        consumerProperties.setProperty("value.deserializer", ByteArrayDeserializer.class.getCanonicalName());
        consumerProperties.setProperty("group.id", "testGroup");
        return consumerProperties;
    }

    private static Pipeline getPipeline(Properties kafkaConsumerProperties, Map<String, Object> serDeProperties) {
        Pipeline p = Pipeline.create();
        StreamSource<Map.Entry<Headers, byte[]>> source = streamFromProcessorWithWatermarks(
                "kafkaSource(" + "inputTopic" + ")",
                true,
                w -> ProcessorMetaSupplier.of(1,
                        StreamKafkaP.processorSupplier(kafkaConsumerProperties, Collections.singletonList("testTopic"),
                                (FunctionEx<ConsumerRecord<? extends byte[], ? extends byte[]>,
                                        Map.Entry<Headers, byte[]>>) r -> entry(r.headers(), r.value()), w)));
        StreamStage<Map.Entry<Headers, byte[]>> streamStage = p.readFrom(source).withoutTimestamps();
        FunctionEx<Map.Entry<Headers, byte[]>, Emp> f1 = e -> {
            AvroDeserializer deserializer = new AvroDeserializer();
            deserializer.configure(serDeProperties, false);
            Thread.sleep(1000);
            return deserializer.deserialize(e.getValue(), new Emp());
        };
        streamStage.map(f1).writeTo(Sinks.logger());
        return p;
    }
}
